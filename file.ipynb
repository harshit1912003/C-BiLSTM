{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Ln75p9vl34c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import time\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r446IL__l6cM",
        "outputId": "2ec32ea8-aa59-4452-9544-384402c9e403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/My Drive/AIL721_A3/Datasets/TrainData.csv')\n",
        "sentences = df['Text'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr0n1WGFl7pE",
        "outputId": "720294bb-51b6-4fba-a168-15066741ed7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3PcJscr5l_wC"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 1026\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "WINDOW_SIZE = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjxGuM2SmA7l",
        "outputId": "749d658e-7975-4f45-9c2e-8846a8ab3377"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jUCMCVFVpHAW"
      },
      "outputs": [],
      "source": [
        "tokenized = [nltk.word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "words = [word for sentence in tokenized for word in sentence]\n",
        "word_counts = Counter(words)\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "vocab.update({word: i+2 for i, (word, _) in enumerate(word_counts.items())})\n",
        "vocab_size = len(vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pHjmgqx0pIZK"
      },
      "outputs": [],
      "source": [
        "def compute_pairs(tokenized, WINDOW_SIZE, vocab):\n",
        "    contexts = []\n",
        "    targets = []\n",
        "    for sentence in tokenized:\n",
        "        for i in range(len(sentence)):\n",
        "            start = max(0, i - WINDOW_SIZE)\n",
        "            end = min(len(sentence), i + WINDOW_SIZE + 1)\n",
        "            context = sentence[start:i] + sentence[i+1:end]\n",
        "            context = context + ['<PAD>'] * (2 * WINDOW_SIZE - len(context))\n",
        "            contexts.append([vocab.get(w, 1) for w in context])\n",
        "            targets.append(vocab.get(sentence[i], 1))\n",
        "    return torch.LongTensor(contexts), torch.LongTensor(targets)\n",
        "\n",
        "contexts, targets = compute_pairs(tokenized, WINDOW_SIZE, vocab)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p64MzEeFpKKL",
        "outputId": "b5df43c0-db86-4aea-880e-f1b371c54a74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "dataset = TensorDataset(contexts, targets)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, v, e):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(v, e)\n",
        "        self.linear = nn.Linear(e, v)\n",
        "\n",
        "    def forward(self, contexts):\n",
        "        embeds = self.embeddings(contexts)\n",
        "        embedcontexts = torch.mean(embeds, dim=1)\n",
        "        out = self.linear(embedcontexts)\n",
        "        # print(contexts.shape, embeds.shape, embedcontexts.shape, out.shape)\n",
        "        return out\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model = CBOW(vocab_size, EMBEDDING_DIM).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iKAVJA8pN-P",
        "outputId": "0ef68b20-cace-4e22-8f96-f2c6827dbcf1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 loss: 4383.8292 time: 18.55s\n",
            "epoch 1 loss: 3546.0027 time: 17.14s\n",
            "epoch 2 loss: 3210.1357 time: 17.91s\n",
            "epoch 3 loss: 2960.9658 time: 17.85s\n",
            "epoch 4 loss: 2759.2934 time: 17.16s\n",
            "epoch 5 loss: 2591.5428 time: 18.47s\n",
            "epoch 6 loss: 2451.6125 time: 19.79s\n",
            "epoch 7 loss: 2334.5250 time: 18.42s\n",
            "epoch 8 loss: 2234.1646 time: 17.57s\n",
            "epoch 9 loss: 2145.9135 time: 17.73s\n",
            "epoch 10 loss: 2066.9322 time: 18.13s\n",
            "epoch 11 loss: 1995.5025 time: 18.25s\n",
            "epoch 12 loss: 1930.3008 time: 18.36s\n",
            "epoch 13 loss: 1870.0006 time: 17.95s\n",
            "epoch 14 loss: 1814.4887 time: 17.74s\n",
            "epoch 15 loss: 1762.7629 time: 20.14s\n",
            "epoch 16 loss: 1714.5636 time: 17.92s\n",
            "epoch 17 loss: 1669.3513 time: 19.31s\n",
            "epoch 18 loss: 1627.0044 time: 17.46s\n",
            "epoch 19 loss: 1587.2834 time: 19.78s\n",
            "epoch 20 loss: 1549.9673 time: 18.06s\n",
            "epoch 21 loss: 1514.8117 time: 18.54s\n",
            "epoch 22 loss: 1481.8163 time: 17.41s\n",
            "epoch 23 loss: 1450.6090 time: 17.50s\n",
            "epoch 24 loss: 1421.1054 time: 17.12s\n",
            "epoch 25 loss: 1393.3914 time: 17.73s\n",
            "epoch 26 loss: 1367.0892 time: 17.77s\n",
            "epoch 27 loss: 1342.4616 time: 17.24s\n",
            "epoch 28 loss: 1319.0232 time: 17.18s\n",
            "epoch 29 loss: 1296.8778 time: 17.25s\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    s = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for contexts, targets in dataloader:\n",
        "        contexts = contexts.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(contexts)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"epoch {epoch} loss: {total_loss:.4f} time: {time.time() - s:.2f}s\")\n",
        "embeddings = model.embeddings.weight.data.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BCrgsQD2pRPl"
      },
      "outputs": [],
      "source": [
        "os.makedirs('/content/drive/My Drive/AIL721_A3/saved/', exist_ok=True)\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/AIL721_A3/saved/cbow_model.pth')\n",
        "torch.save(vocab, '/content/drive/My Drive/AIL721_A3/saved/vocab.pth')\n",
        "embeddings = model.embeddings.weight.data.cpu().numpy()\n",
        "np.save('/content/drive/My Drive/AIL721_A3/saved/embeddings.npy', embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "slqCbnOkKZ4r"
      },
      "outputs": [],
      "source": [
        "vocab = torch.load('/content/drive/My Drive/AIL721_A3/saved/vocab.pth')\n",
        "\n",
        "loaded_model = CBOW(len(vocab), EMBEDDING_DIM).to(device)\n",
        "\n",
        "loaded_model.load_state_dict(torch.load('/content/drive/My Drive/AIL721_A3/saved/cbow_model.pth'))\n",
        "loaded_model.eval()\n",
        "\n",
        "len(vocab)\n",
        "embedding_matrix = np.load('/content/drive/My Drive/AIL721_A3/saved/embeddings.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AvCSd00IqO6N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uyenPy7ZqO3U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "79W2WX9tqOy3"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = embedding_matrix.shape[1]\n",
        "NUM_CLASSES = len(df['Category'].unique())\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "LSTM_HIDDEN_DIM = 128\n",
        "DENSE_EMBED_DIM = 128\n",
        "CONV_OUT_DIM = 100\n",
        "FUSION_DIM = 256\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv3uTr5AqTSn",
        "outputId": "7753588d-a76e-4022-e602-fb567b262a46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1490"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized = [nltk.word_tokenize(text.lower()) for text in df['Text']]\n",
        "sequences = [[vocab.get(word, 1) for word in seq] for seq in tokenized]\n",
        "\n",
        "len(sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa_nvleVqUd5",
        "outputId": "eefdf41d-9279-426a-aabf-56506b5c4353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3496"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MAX_LEN = max(len(sequence) for sequence in sequences)\n",
        "MAX_LEN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ii-GjWEZqVkf"
      },
      "outputs": [],
      "source": [
        "padded = torch.zeros((len(sequences), MAX_LEN), dtype=torch.long)\n",
        "for i, seq in enumerate(sequences):\n",
        "    length = min(len(seq), MAX_LEN)\n",
        "    padded[i,:length] = torch.tensor(seq[:length])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cxCPAp_jqXBM"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(df['Category'])\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "z2P4yACZqYPT"
      },
      "outputs": [],
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9h6UmlZHrgmi"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
        "    padded, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NbcF7ES8rlIn"
      },
      "outputs": [],
      "source": [
        "dataset_train = NewsDataset(train_sequences, train_labels)\n",
        "dataset_test = NewsDataset(test_sequences, test_labels)\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za5Ein8-tzUy"
      },
      "source": [
        "# NOTE: I had to run the following model again while I was cleaning this file, and the new test set accuracy is 97 instead of 94, I think its due to randomness so wont be utilizing this fact in the report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIVxiO3AKyXo"
      },
      "outputs": [],
      "source": [
        "class CLSTM_WE(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.v, self.e = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix),\n",
        "            freeze=False\n",
        "        )\n",
        "        self.conv3 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=3)\n",
        "        self.conv4 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=4)\n",
        "        self.conv5 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=5)\n",
        "        self.cnn_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.e, LSTM_HIDDEN_DIM, bidirectional=False, batch_first=True)\n",
        "        self.lstm_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.embed_dense = nn.Linear(self.e, DENSE_EMBED_DIM)\n",
        "\n",
        "        self.fc1 = nn.Linear(CONV_OUT_DIM*3 + LSTM_HIDDEN_DIM + DENSE_EMBED_DIM, FUSION_DIM)\n",
        "        self.fc2 = nn.Linear(FUSION_DIM, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        x_embed = self.embedding(x)\n",
        "\n",
        "        x_cnn = x_embed.permute(0, 2, 1)\n",
        "        c3 = torch.relu(self.conv3(x_cnn)).max(dim=2)[0]\n",
        "        c4 = torch.relu(self.conv4(x_cnn)).max(dim=2)[0]\n",
        "        c5 = torch.relu(self.conv5(x_cnn)).max(dim=2)[0]\n",
        "        cnn_out = torch.cat([c3, c4, c5], dim=1)\n",
        "        cnn_out = self.cnn_dropout(cnn_out)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x_embed)\n",
        "        lstm_out = lstm_out.mean(dim=1)\n",
        "        lstm_out = self.lstm_dropout(lstm_out)\n",
        "\n",
        "        embed_out = x_embed.mean(dim=1)\n",
        "        embed_out = self.embed_dense(embed_out)\n",
        "\n",
        "        combined = torch.cat([cnn_out, lstm_out, embed_out], dim=1)\n",
        "        out = self.dropout(torch.relu(self.fc1(combined)))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4n6vlglMMGN"
      },
      "outputs": [],
      "source": [
        "model = CLSTM_WE(embedding_matrix, NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01uVqbvvODbb",
        "outputId": "b3bcc6af-4c8f-46e3-b4f5-a8e4576b640d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, loss: 1.6896 time: 7.10 acc: 0.2307\n",
            "epoch 1, loss: 1.3923 time: 7.23 acc: 0.3859\n",
            "epoch 2, loss: 1.0026 time: 7.28 acc: 0.6166\n",
            "epoch 3, loss: 0.6660 time: 7.10 acc: 0.7592\n",
            "epoch 4, loss: 0.4679 time: 6.91 acc: 0.8272\n",
            "epoch 5, loss: 0.2694 time: 6.80 acc: 0.9102\n",
            "epoch 6, loss: 0.2267 time: 6.74 acc: 0.9270\n",
            "epoch 7, loss: 0.1430 time: 6.69 acc: 0.9505\n",
            "epoch 8, loss: 0.1033 time: 6.67 acc: 0.9698\n",
            "epoch 9, loss: 0.0839 time: 6.70 acc: 0.9757\n",
            "Test Accuracy: 0.9765\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    s = time.time()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'epoch {epoch}, loss: {total_loss/len(train_loader):.4f} time: {time.time()-s:.2f} acc: {correct/total:.4f}')\n",
        "\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/AIL721_A3/CLSTM_WE.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kBFX2m2RB-o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EKveIh-GDyOv"
      },
      "outputs": [],
      "source": [
        "ATTN_DIM = 100\n",
        "\n",
        "class CBiLSTM_WE_SA(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_classes):\n",
        "        super().__init__()\n",
        "        self.v, self.e = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix),\n",
        "            freeze=False\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=3)\n",
        "        self.conv4 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=4)\n",
        "        self.conv5 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=5)\n",
        "        self.cnn_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.e, LSTM_HIDDEN_DIM, bidirectional=True, batch_first=True)\n",
        "        self.lstm_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.embed_dense = nn.Linear(self.e, DENSE_EMBED_DIM)\n",
        "\n",
        "        self.attn_dim = ATTN_DIM\n",
        "        self.proj_cnn = nn.Linear(CONV_OUT_DIM * 3, self.attn_dim)\n",
        "        self.proj_lstm = nn.Linear(2 * LSTM_HIDDEN_DIM, self.attn_dim)\n",
        "        self.proj_embed = nn.Linear(DENSE_EMBED_DIM, self.attn_dim)\n",
        "\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim=self.attn_dim, num_heads=4)\n",
        "\n",
        "        self.fc2 = nn.Linear(self.attn_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_embed = self.embedding(x)\n",
        "\n",
        "        x_cnn = x_embed.permute(0, 2, 1)\n",
        "        c3 = torch.relu(self.conv3(x_cnn)).max(dim=2)[0]\n",
        "        c4 = torch.relu(self.conv4(x_cnn)).max(dim=2)[0]\n",
        "        c5 = torch.relu(self.conv5(x_cnn)).max(dim=2)[0]\n",
        "        cnn_out = torch.cat([c3, c4, c5], dim=1)\n",
        "        cnn_out = self.cnn_dropout(cnn_out)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x_embed)\n",
        "        lstm_out = lstm_out.mean(dim=1)\n",
        "        lstm_out = self.lstm_dropout(lstm_out)\n",
        "\n",
        "        embed_out = x_embed.mean(dim=1)\n",
        "        embed_out = self.embed_dense(embed_out)\n",
        "\n",
        "        cnn_token = self.proj_cnn(cnn_out)\n",
        "        lstm_token = self.proj_lstm(lstm_out)\n",
        "        embed_token = self.proj_embed(embed_out)\n",
        "\n",
        "        tokens = torch.stack([cnn_token, lstm_token, embed_token], dim=1)\n",
        "\n",
        "        tokens = tokens.permute(1, 0, 2)\n",
        "        attn_output, _ = self.self_attn(tokens, tokens, tokens)\n",
        "\n",
        "        aggregated = attn_output.mean(dim=0)\n",
        "        out = self.dropout(aggregated)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBaMf054ODYz",
        "outputId": "2253e8bd-0325-44b5-d54a-395b8607ac38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, loss: 1.5852 time: 18.56 acc: 0.2693\n",
            "epoch 1, loss: 1.1371 time: 8.32 acc: 0.5487\n",
            "epoch 2, loss: 0.6258 time: 8.34 acc: 0.7718\n",
            "epoch 3, loss: 0.3503 time: 8.42 acc: 0.8867\n",
            "epoch 4, loss: 0.1969 time: 8.51 acc: 0.9295\n",
            "epoch 5, loss: 0.1552 time: 8.49 acc: 0.9446\n",
            "epoch 6, loss: 0.1226 time: 8.43 acc: 0.9622\n",
            "epoch 7, loss: 0.1016 time: 8.36 acc: 0.9622\n",
            "epoch 8, loss: 0.0770 time: 8.31 acc: 0.9757\n",
            "epoch 9, loss: 0.0438 time: 8.37 acc: 0.9815\n",
            "Test Accuracy: 0.9698\n"
          ]
        }
      ],
      "source": [
        "dataset_train = NewsDataset(train_sequences, train_labels)\n",
        "dataset_test = NewsDataset(test_sequences, test_labels)\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model = CBiLSTM_WE_SA(embedding_matrix, NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "import time\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    s = time.time()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'epoch {epoch}, loss: {total_loss/len(train_loader):.4f} time: {time.time()-s:.2f} acc: {correct/total:.4f}')\n",
        "\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/AIL721_A3/CBiLSTM_WE_SA.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcHe5At6GPIJ",
        "outputId": "255e8e39-a54a-4735-e69f-4f81b719a55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mbZ8-VOGYFD"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HzCssjqGIp9",
        "outputId": "457d69e6-808d-4da8-ac2b-4cfc7078ce7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Embedding matrix shape: (28181, 300)\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "embedding_dim = model.vector_size\n",
        "\n",
        "embedding_matrix2 = np.zeros((len(vocab) + 1, embedding_dim))\n",
        "\n",
        "for word, idx in vocab.items():\n",
        "    if word in model.key_to_index:\n",
        "        embedding_matrix2[idx] = model[word]\n",
        "    else:\n",
        "        embedding_matrix2[idx] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
        "\n",
        "print(embedding_matrix2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irTnai49FkTa"
      },
      "outputs": [],
      "source": [
        "class CBiLSTM_WE_ME(nn.Module):\n",
        "    def __init__(self, embedding_matrix1, embedding_matrix2, num_classes):\n",
        "        super().__init__()\n",
        "        self.v1, self.e = embedding_matrix1.shape\n",
        "        self.v2, _ = embedding_matrix2.shape\n",
        "\n",
        "        self.embedding1 = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix1),\n",
        "            freeze=False\n",
        "        )\n",
        "        self.embedding2 = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix2),\n",
        "            freeze=False\n",
        "        )\n",
        "\n",
        "        self.gate1 = nn.Linear(self.e, 1)\n",
        "        self.gate2 = nn.Linear(self.e, 1)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=3)\n",
        "        self.conv4 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=4)\n",
        "        self.conv5 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=5)\n",
        "        self.cnn_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.e, LSTM_HIDDEN_DIM, bidirectional=True, batch_first=True)\n",
        "        self.lstm_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.embed_dense = nn.Linear(self.e, DENSE_EMBED_DIM)\n",
        "\n",
        "        self.fc1 = nn.Linear(CONV_OUT_DIM * 3 + 2 * LSTM_HIDDEN_DIM + DENSE_EMBED_DIM, FUSION_DIM)\n",
        "        self.fc2 = nn.Linear(FUSION_DIM, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        emb1 = self.embedding1(x)\n",
        "        emb2 = self.embedding2(x)\n",
        "\n",
        "        gate1 = self.gate1(emb1)\n",
        "        gate2 = self.gate2(emb2)\n",
        "        gates = torch.cat([gate1, gate2], dim=2)\n",
        "        weights = torch.softmax(gates, dim=2)\n",
        "\n",
        "        meta_emb = weights[:, :, 0:1] * emb1 + weights[:, :, 1:2] * emb2\n",
        "\n",
        "        x_cnn = meta_emb.permute(0, 2, 1)\n",
        "        c3 = torch.relu(self.conv3(x_cnn)).max(dim=2)[0]\n",
        "        c4 = torch.relu(self.conv4(x_cnn)).max(dim=2)[0]\n",
        "        c5 = torch.relu(self.conv5(x_cnn)).max(dim=2)[0]\n",
        "        cnn_out = torch.cat([c3, c4, c5], dim=1)\n",
        "        cnn_out = self.cnn_dropout(cnn_out)\n",
        "\n",
        "        lstm_out, _ = self.lstm(meta_emb)\n",
        "        lstm_out = lstm_out.mean(dim=1)\n",
        "        lstm_out = self.lstm_dropout(lstm_out)\n",
        "\n",
        "        embed_out = meta_emb.mean(dim=1)\n",
        "        embed_out = self.embed_dense(embed_out)\n",
        "\n",
        "        combined = torch.cat([cnn_out, lstm_out, embed_out], dim=1)\n",
        "        out = self.dropout(torch.relu(self.fc1(combined)))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x1tZ97ZFkTb",
        "outputId": "2a86a452-673c-48ed-8c28-ae486a8d3a74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, loss: 1.6233 time: 20.19 acc: 0.2483\n",
            "epoch 1, loss: 1.3379 time: 9.27 acc: 0.4530\n",
            "epoch 2, loss: 0.8365 time: 9.37 acc: 0.7039\n",
            "epoch 3, loss: 0.5183 time: 9.43 acc: 0.8129\n",
            "epoch 4, loss: 0.2948 time: 9.43 acc: 0.9086\n",
            "epoch 5, loss: 0.1840 time: 9.41 acc: 0.9471\n",
            "epoch 6, loss: 0.1272 time: 9.37 acc: 0.9513\n",
            "epoch 7, loss: 0.0956 time: 9.31 acc: 0.9706\n",
            "epoch 8, loss: 0.0681 time: 9.28 acc: 0.9782\n",
            "epoch 9, loss: 0.0503 time: 9.29 acc: 0.9883\n",
            "Test Accuracy: 0.9631\n"
          ]
        }
      ],
      "source": [
        "dataset_train = NewsDataset(train_sequences, train_labels)\n",
        "dataset_test = NewsDataset(test_sequences, test_labels)\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model = CBiLSTM_WE_ME(embedding_matrix, embedding_matrix2, NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "import time\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    s = time.time()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'epoch {epoch}, loss: {total_loss/len(train_loader):.4f} time: {time.time()-s:.2f} acc: {correct/total:.4f}')\n",
        "\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/AIL721_A3/CBiLSTM_WE_ME.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL57uaBYUQPu"
      },
      "outputs": [],
      "source": [
        "class CLSTM_WE_ME_SA(nn.Module):\n",
        "    def __init__(self, embedding_matrix1, embedding_matrix2, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.v1, self.e = embedding_matrix1.shape\n",
        "        self.v2, _ = embedding_matrix2.shape\n",
        "\n",
        "        self.embedding1 = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix1), freeze=False\n",
        "        )\n",
        "        self.embedding2 = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix2), freeze=False\n",
        "        )\n",
        "        self.gate1 = nn.Linear(self.e, 1)\n",
        "        self.gate2 = nn.Linear(self.e, 1)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=3)\n",
        "        self.conv4 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=4)\n",
        "        self.conv5 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=5)\n",
        "        self.cnn_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.e, LSTM_HIDDEN_DIM, bidirectional=True, batch_first=True)\n",
        "        self.lstm_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.embed_dense = nn.Linear(self.e, DENSE_EMBED_DIM)\n",
        "\n",
        "        self.attn_dim = FUSION_DIM\n",
        "        self.proj_cnn = nn.Linear(CONV_OUT_DIM * 3, self.attn_dim)\n",
        "        self.proj_lstm = nn.Linear(2 * LSTM_HIDDEN_DIM, self.attn_dim)\n",
        "        self.proj_embed = nn.Linear(DENSE_EMBED_DIM, self.attn_dim)\n",
        "\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim=self.attn_dim, num_heads=4)\n",
        "\n",
        "        self.fc2 = nn.Linear(self.attn_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        emb1 = self.embedding1(x)\n",
        "        emb2 = self.embedding2(x)\n",
        "\n",
        "        gate1 = self.gate1(emb1)\n",
        "        gate2 = self.gate2(emb2)\n",
        "        gates = torch.cat([gate1, gate2], dim=2)\n",
        "        weights = torch.softmax(gates, dim=2)\n",
        "        meta_emb = weights[:, :, 0:1] * emb1 + weights[:, :, 1:2] * emb2\n",
        "\n",
        "        x_cnn = meta_emb.permute(0, 2, 1)\n",
        "        c3 = torch.relu(self.conv3(x_cnn)).max(dim=2)[0]\n",
        "        c4 = torch.relu(self.conv4(x_cnn)).max(dim=2)[0]\n",
        "        c5 = torch.relu(self.conv5(x_cnn)).max(dim=2)[0]\n",
        "        cnn_out = torch.cat([c3, c4, c5], dim=1)\n",
        "        cnn_out = self.cnn_dropout(cnn_out)\n",
        "\n",
        "        lstm_out, _ = self.lstm(meta_emb)\n",
        "        lstm_out = lstm_out.mean(dim=1)\n",
        "        lstm_out = self.lstm_dropout(lstm_out)\n",
        "\n",
        "        embed_out = meta_emb.mean(dim=1)\n",
        "        embed_out = self.embed_dense(embed_out)\n",
        "\n",
        "        cnn_token = self.proj_cnn(cnn_out)\n",
        "        lstm_token = self.proj_lstm(lstm_out)\n",
        "        embed_token = self.proj_embed(embed_out)\n",
        "\n",
        "        tokens = torch.stack([cnn_token, lstm_token, embed_token], dim=1)\n",
        "        tokens = tokens.permute(1, 0, 2)\n",
        "\n",
        "        attn_output, _ = self.self_attn(tokens, tokens, tokens)\n",
        "        aggregated = attn_output.mean(dim=0)\n",
        "\n",
        "        out = self.dropout(aggregated)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5IM_9ANU6oV",
        "outputId": "51e18045-3c97-4180-f38c-40ebaa473c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, loss: 1.5235 time: 20.74 acc: 0.2978\n",
            "epoch 1, loss: 0.7981 time: 9.77 acc: 0.6988\n",
            "epoch 2, loss: 0.3563 time: 9.80 acc: 0.8876\n",
            "epoch 3, loss: 0.2082 time: 10.00 acc: 0.9413\n",
            "epoch 4, loss: 0.1252 time: 10.10 acc: 0.9488\n",
            "epoch 5, loss: 0.1137 time: 10.10 acc: 0.9555\n",
            "epoch 6, loss: 0.1002 time: 9.98 acc: 0.9715\n",
            "epoch 7, loss: 0.0872 time: 9.90 acc: 0.9757\n",
            "epoch 8, loss: 0.0540 time: 9.82 acc: 0.9849\n",
            "epoch 9, loss: 0.0586 time: 9.85 acc: 0.9773\n",
            "Test Accuracy: 0.9564\n"
          ]
        }
      ],
      "source": [
        "dataset_train = NewsDataset(train_sequences, train_labels)\n",
        "dataset_test = NewsDataset(test_sequences, test_labels)\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model = CLSTM_WE_ME_SA(embedding_matrix, embedding_matrix2, NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "import time\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    s = time.time()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'epoch {epoch}, loss: {total_loss/len(train_loader):.4f} time: {time.time()-s:.2f} acc: {correct/total:.4f}')\n",
        "\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/AIL721_A3/CLSTM_WE_ME_SA.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFXLuK8IVPPf",
        "outputId": "4f7c6163-4bfa-4255-fa99-23de983bdcf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, loss: 0.0481 time: 9.85 acc: 0.9891\n",
            "Test Accuracy: 0.9631\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    s = time.time()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'epoch {epoch}, loss: {total_loss/len(train_loader):.4f} time: {time.time()-s:.2f} acc: {correct/total:.4f}')\n",
        "\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/AIL721_A3/CLSTM_WE_ME_SA.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h2hbRZkWSl1"
      },
      "outputs": [],
      "source": [
        "ATTN_DIM = 100\n",
        "\n",
        "class CBiLSTM_WE_SA_T(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_classes):\n",
        "        super().__init__()\n",
        "        self.v, self.e = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix),\n",
        "            freeze=False\n",
        "        )\n",
        "\n",
        "        T_enc = nn.TransformerEncoderLayer(\n",
        "            d_model=self.e,\n",
        "            nhead=4,\n",
        "            dim_feedforward=2048\n",
        "        )\n",
        "        self.trans_enc = nn.TransformerEncoder(T_enc, num_layers=1)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=3)\n",
        "        self.conv4 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=4)\n",
        "        self.conv5 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=5)\n",
        "        self.cnn_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.e, LSTM_HIDDEN_DIM, bidirectional=True, batch_first=True)\n",
        "        self.lstm_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.embed_dense = nn.Linear(self.e, DENSE_EMBED_DIM)\n",
        "\n",
        "        self.attn_dim = ATTN_DIM\n",
        "        self.proj_cnn = nn.Linear(CONV_OUT_DIM * 3, self.attn_dim)\n",
        "        self.proj_lstm = nn.Linear(2 * LSTM_HIDDEN_DIM, self.attn_dim)\n",
        "        self.proj_embed = nn.Linear(DENSE_EMBED_DIM, self.attn_dim)\n",
        "\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim=self.attn_dim, num_heads=4)\n",
        "\n",
        "        self.fc2 = nn.Linear(self.attn_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_embed = self.embedding(x)\n",
        "        x_enc = self.trans_enc(x_embed.permute(1, 0, 2)).permute(1, 0, 2)\n",
        "\n",
        "        x_cnn = x_enc.permute(0, 2, 1)\n",
        "        c3 = torch.relu(self.conv3(x_cnn)).max(dim=2)[0]\n",
        "        c4 = torch.relu(self.conv4(x_cnn)).max(dim=2)[0]\n",
        "        c5 = torch.relu(self.conv5(x_cnn)).max(dim=2)[0]\n",
        "        cnn_out = torch.cat([c3, c4, c5], dim=1)\n",
        "        cnn_out = self.cnn_dropout(cnn_out)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x_enc)\n",
        "        lstm_out = lstm_out.mean(dim=1)\n",
        "        lstm_out = self.lstm_dropout(lstm_out)\n",
        "\n",
        "        embed_out = x_enc.mean(dim=1)\n",
        "        embed_out = self.embed_dense(embed_out)\n",
        "\n",
        "        cnn_token = self.proj_cnn(cnn_out)\n",
        "        lstm_token = self.proj_lstm(lstm_out)\n",
        "        embed_token = self.proj_embed(embed_out)\n",
        "\n",
        "        tokens = torch.stack([cnn_token, lstm_token, embed_token], dim=1)\n",
        "\n",
        "        tokens = tokens.permute(1, 0, 2)\n",
        "        attn_output, _ = self.self_attn(tokens, tokens, tokens)\n",
        "\n",
        "        aggregated = attn_output.mean(dim=0)\n",
        "        out = self.dropout(aggregated)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEcD_7dMWSl2",
        "outputId": "ad213d90-0c35-4d3c-fa4e-876e00f6d056"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, loss: 1.6248 time: 74.92 acc: 0.2156\n",
            "epoch 1, loss: 1.4452 time: 71.81 acc: 0.3238\n",
            "epoch 2, loss: 1.0107 time: 71.99 acc: 0.5789\n",
            "epoch 3, loss: 0.5512 time: 72.18 acc: 0.8037\n",
            "epoch 4, loss: 0.3083 time: 72.09 acc: 0.9111\n",
            "epoch 5, loss: 0.1879 time: 72.02 acc: 0.9438\n",
            "epoch 6, loss: 0.1299 time: 72.13 acc: 0.9681\n",
            "epoch 7, loss: 0.1397 time: 72.12 acc: 0.9648\n",
            "epoch 8, loss: 0.1218 time: 71.96 acc: 0.9639\n",
            "epoch 9, loss: 0.1880 time: 71.90 acc: 0.9614\n",
            "Test Accuracy: 0.9060\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "dataset_train = NewsDataset(train_sequences, train_labels)\n",
        "dataset_test = NewsDataset(test_sequences, test_labels)\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model = CBiLSTM_WE_SA_T(embedding_matrix, NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "import time\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    s = time.time()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'epoch {epoch}, loss: {total_loss/len(train_loader):.4f} time: {time.time()-s:.2f} acc: {correct/total:.4f}')\n",
        "\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/AIL721_A3/CBiLSTM_WE_SA_T.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuTqtNG0cr6K"
      },
      "outputs": [],
      "source": [
        "ATTN_DIM = 100\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class CBiLSTM_WE_SA_T_PE(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_classes):\n",
        "        super().__init__()\n",
        "        self.v, self.e = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix),\n",
        "            freeze=False\n",
        "        )\n",
        "\n",
        "        position = torch.arange(MAX_LEN).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, self.e, 2) * (-math.log(10000.0) / self.e))\n",
        "        pe = torch.zeros(MAX_LEN, 1, self.e)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "        T_enc = nn.TransformerEncoderLayer(\n",
        "            d_model=self.e,\n",
        "            nhead=4,\n",
        "            dim_feedforward=2048\n",
        "        )\n",
        "        self.trans_enc = nn.TransformerEncoder(T_enc, num_layers=1)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=3)\n",
        "        self.conv4 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=4)\n",
        "        self.conv5 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=5)\n",
        "        self.cnn_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.e, LSTM_HIDDEN_DIM, bidirectional=True, batch_first=True)\n",
        "        self.lstm_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.embed_dense = nn.Linear(self.e, DENSE_EMBED_DIM)\n",
        "\n",
        "        self.attn_dim = ATTN_DIM\n",
        "        self.proj_cnn = nn.Linear(CONV_OUT_DIM * 3, self.attn_dim)\n",
        "        self.proj_lstm = nn.Linear(2 * LSTM_HIDDEN_DIM, self.attn_dim)\n",
        "        self.proj_embed = nn.Linear(DENSE_EMBED_DIM, self.attn_dim)\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim=self.attn_dim, num_heads=4)\n",
        "\n",
        "        self.fc2 = nn.Linear(self.attn_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_embed = self.embedding(x)\n",
        "        x_embed_permuted = x_embed.permute(1, 0, 2)\n",
        "        seq_len = x_embed_permuted.size(0)\n",
        "\n",
        "        x_embedpos = x_embed_permuted + self.pe[:seq_len]\n",
        "\n",
        "        # x_enc = self.trans_enc(x_embedpos.permute(1, 0, 2)).permute(1, 0, 2)\n",
        "        x_enc = self.trans_enc(x_embedpos)\n",
        "        x_enc = x_enc.permute(1, 0, 2)\n",
        "\n",
        "        x_cnn = x_enc.permute(0, 2, 1)\n",
        "        c3 = torch.relu(self.conv3(x_cnn)).max(dim=2)[0]\n",
        "        c4 = torch.relu(self.conv4(x_cnn)).max(dim=2)[0]\n",
        "        c5 = torch.relu(self.conv5(x_cnn)).max(dim=2)[0]\n",
        "        cnn_out = torch.cat([c3, c4, c5], dim=1)\n",
        "        cnn_out = self.cnn_dropout(cnn_out)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x_enc)\n",
        "        lstm_out = lstm_out.mean(dim=1)\n",
        "        lstm_out = self.lstm_dropout(lstm_out)\n",
        "\n",
        "        embed_out = x_enc.mean(dim=1)\n",
        "        embed_out = self.embed_dense(embed_out)\n",
        "\n",
        "        cnn_token = self.proj_cnn(cnn_out)\n",
        "        lstm_token = self.proj_lstm(lstm_out)\n",
        "        embed_token = self.proj_embed(embed_out)\n",
        "\n",
        "        tokens = torch.stack([cnn_token, lstm_token, embed_token], dim=1)\n",
        "        tokens = tokens.permute(1, 0, 2)\n",
        "        attn_output, _ = self.self_attn(tokens, tokens, tokens)\n",
        "\n",
        "        aggregated = attn_output.mean(dim=0)\n",
        "        out = self.dropout(aggregated)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qANc6SCtbMWB",
        "outputId": "914e5f82-5a96-48e3-a925-ed32f7b966af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, loss: 1.6185 time: 76.17 acc: 0.2391\n",
            "epoch 1, loss: 1.3244 time: 74.76 acc: 0.4312\n",
            "epoch 2, loss: 0.9060 time: 76.02 acc: 0.6602\n",
            "epoch 3, loss: 0.6128 time: 76.88 acc: 0.7945\n",
            "epoch 4, loss: 0.4445 time: 77.02 acc: 0.8624\n",
            "epoch 5, loss: 0.3372 time: 76.93 acc: 0.8935\n",
            "epoch 6, loss: 0.3436 time: 76.91 acc: 0.9018\n",
            "epoch 7, loss: 0.2683 time: 76.69 acc: 0.9253\n",
            "epoch 8, loss: 0.2393 time: 76.63 acc: 0.9329\n",
            "epoch 9, loss: 0.2159 time: 76.73 acc: 0.9379\n",
            "Test Accuracy: 0.8758\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "dataset_train = NewsDataset(train_sequences, train_labels)\n",
        "dataset_test = NewsDataset(test_sequences, test_labels)\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model = CBiLSTM_WE_SA_T_PE(embedding_matrix, NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "import time\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    s = time.time()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'epoch {epoch}, loss: {total_loss/len(train_loader):.4f} time: {time.time()-s:.2f} acc: {correct/total:.4f}')\n",
        "\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/AIL721_A3/CBiLSTM_WE_SA_T_PE.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhzHu1oF9ADG"
      },
      "outputs": [],
      "source": [
        "ATTN_DIM = 100\n",
        "\n",
        "class CBiLSTM_WE_SA_5T(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_classes):\n",
        "        super().__init__()\n",
        "        self.v, self.e = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix),\n",
        "            freeze=False\n",
        "        )\n",
        "\n",
        "        T_enc = nn.TransformerEncoderLayer(\n",
        "            d_model=self.e,\n",
        "            nhead=4,\n",
        "            dim_feedforward=2048\n",
        "        )\n",
        "\n",
        "        self.trans_enc = nn.TransformerEncoder(T_enc, num_layers=5)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=3)\n",
        "        self.conv4 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=4)\n",
        "        self.conv5 = nn.Conv1d(self.e, CONV_OUT_DIM, kernel_size=5)\n",
        "        self.cnn_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.e, LSTM_HIDDEN_DIM, bidirectional=True, batch_first=True)\n",
        "        self.lstm_dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.embed_dense = nn.Linear(self.e, DENSE_EMBED_DIM)\n",
        "\n",
        "        self.attn_dim = ATTN_DIM\n",
        "        self.proj_cnn = nn.Linear(CONV_OUT_DIM * 3, self.attn_dim)\n",
        "        self.proj_lstm = nn.Linear(2 * LSTM_HIDDEN_DIM, self.attn_dim)\n",
        "        self.proj_embed = nn.Linear(DENSE_EMBED_DIM, self.attn_dim)\n",
        "\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim=self.attn_dim, num_heads=4)\n",
        "\n",
        "        self.fc2 = nn.Linear(self.attn_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_embed = self.embedding(x)\n",
        "        x_enc = self.trans_enc(x_embed.permute(1, 0, 2)).permute(1, 0, 2)\n",
        "\n",
        "        x_cnn = x_enc.permute(0, 2, 1)\n",
        "        c3 = torch.relu(self.conv3(x_cnn)).max(dim=2)[0]\n",
        "        c4 = torch.relu(self.conv4(x_cnn)).max(dim=2)[0]\n",
        "        c5 = torch.relu(self.conv5(x_cnn)).max(dim=2)[0]\n",
        "        cnn_out = torch.cat([c3, c4, c5], dim=1)\n",
        "        cnn_out = self.cnn_dropout(cnn_out)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x_enc)\n",
        "        lstm_out = lstm_out.mean(dim=1)\n",
        "        lstm_out = self.lstm_dropout(lstm_out)\n",
        "\n",
        "        embed_out = x_enc.mean(dim=1)\n",
        "        embed_out = self.embed_dense(embed_out)\n",
        "\n",
        "        cnn_token = self.proj_cnn(cnn_out)\n",
        "        lstm_token = self.proj_lstm(lstm_out)\n",
        "        embed_token = self.proj_embed(embed_out)\n",
        "\n",
        "        tokens = torch.stack([cnn_token, lstm_token, embed_token], dim=1)\n",
        "\n",
        "        tokens = tokens.permute(1, 0, 2)\n",
        "        attn_output, _ = self.self_attn(tokens, tokens, tokens)\n",
        "\n",
        "        aggregated = attn_output.mean(dim=0)\n",
        "        out = self.dropout(aggregated)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrgLEAOJ9ADH",
        "outputId": "78a223fd-564f-4e68-f7cb-7c4b34c5d865"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, loss: 1.6309 time: 341.04 acc: 0.2156\n",
            "epoch 1, loss: 1.6203 time: 345.31 acc: 0.2122\n",
            "epoch 2, loss: 1.6135 time: 345.80 acc: 0.2139\n",
            "epoch 3, loss: 1.6105 time: 345.22 acc: 0.2500\n",
            "epoch 4, loss: 1.6137 time: 345.45 acc: 0.2164\n",
            "epoch 5, loss: 1.6067 time: 345.34 acc: 0.2248\n",
            "epoch 6, loss: 1.6136 time: 345.60 acc: 0.2072\n",
            "epoch 7, loss: 1.6102 time: 345.10 acc: 0.2282\n",
            "epoch 8, loss: 1.6095 time: 344.77 acc: 0.2198\n",
            "epoch 9, loss: 1.6065 time: 344.60 acc: 0.2232\n",
            "Test Accuracy: 0.2248\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "dataset_train = NewsDataset(train_sequences, train_labels)\n",
        "dataset_test = NewsDataset(test_sequences, test_labels)\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model = CBiLSTM_WE_SA_5T(embedding_matrix, NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "import time\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    s = time.time()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'epoch {epoch}, loss: {total_loss/len(train_loader):.4f} time: {time.time()-s:.2f} acc: {correct/total:.4f}')\n",
        "\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/AIL721_A3/CBiLSTM_WE_SA_5T.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I86mtffyODZ_"
      },
      "source": [
        "# Final architecture "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "RoOEZdo9NpSK"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/AIL721_A3/Datasets/TestLabels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "fJ7ooSOvQ2ox",
        "outputId": "fcaeceff-84a5-4656-c041-12a50ffa6a05"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-430f2f1e-95b7-4d7e-bfc6-6666b04b20f1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleId</th>\n",
              "      <th>Text</th>\n",
              "      <th>Label - (business, tech, politics, sport, entertainment)</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "      <th>Unnamed: 5</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>...</th>\n",
              "      <th>Unnamed: 20</th>\n",
              "      <th>Unnamed: 21</th>\n",
              "      <th>Unnamed: 22</th>\n",
              "      <th>Unnamed: 23</th>\n",
              "      <th>Unnamed: 24</th>\n",
              "      <th>Unnamed: 25</th>\n",
              "      <th>Unnamed: 26</th>\n",
              "      <th>Unnamed: 27</th>\n",
              "      <th>Unnamed: 28</th>\n",
              "      <th>Unnamed: 29</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1018</td>\n",
              "      <td>qpr keeper day heads for preston queens park r...</td>\n",
              "      <td>sport</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1319</td>\n",
              "      <td>software watching while you work software that...</td>\n",
              "      <td>tech</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1138</td>\n",
              "      <td>d arcy injury adds to ireland woe gordon d arc...</td>\n",
              "      <td>sport</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>459</td>\n",
              "      <td>india s reliance family feud heats up the ongo...</td>\n",
              "      <td>business</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1020</td>\n",
              "      <td>boro suffer morrison injury blow middlesbrough...</td>\n",
              "      <td>sport</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>730</th>\n",
              "      <td>1923</td>\n",
              "      <td>eu to probe alitalia  state aid  the european ...</td>\n",
              "      <td>business</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>731</th>\n",
              "      <td>373</td>\n",
              "      <td>u2 to play at grammy awards show irish rock ba...</td>\n",
              "      <td>entertainment</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>732</th>\n",
              "      <td>1704</td>\n",
              "      <td>sport betting rules in spotlight a group of mp...</td>\n",
              "      <td>sport</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>733</th>\n",
              "      <td>206</td>\n",
              "      <td>alfa romeos  to get gm engines  fiat is to sto...</td>\n",
              "      <td>business</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>734</th>\n",
              "      <td>471</td>\n",
              "      <td>citizenship event for 18s touted citizenship c...</td>\n",
              "      <td>politics</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>735 rows  30 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-430f2f1e-95b7-4d7e-bfc6-6666b04b20f1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-430f2f1e-95b7-4d7e-bfc6-6666b04b20f1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-430f2f1e-95b7-4d7e-bfc6-6666b04b20f1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7197bd2b-639c-46d0-9ae7-74082bc89889\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7197bd2b-639c-46d0-9ae7-74082bc89889')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7197bd2b-639c-46d0-9ae7-74082bc89889 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d75c01a7-3172-460e-8692-849f7199d49d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d75c01a7-3172-460e-8692-849f7199d49d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     ArticleId                                               Text  \\\n",
              "0         1018  qpr keeper day heads for preston queens park r...   \n",
              "1         1319  software watching while you work software that...   \n",
              "2         1138  d arcy injury adds to ireland woe gordon d arc...   \n",
              "3          459  india s reliance family feud heats up the ongo...   \n",
              "4         1020  boro suffer morrison injury blow middlesbrough...   \n",
              "..         ...                                                ...   \n",
              "730       1923  eu to probe alitalia  state aid  the european ...   \n",
              "731        373  u2 to play at grammy awards show irish rock ba...   \n",
              "732       1704  sport betting rules in spotlight a group of mp...   \n",
              "733        206  alfa romeos  to get gm engines  fiat is to sto...   \n",
              "734        471  citizenship event for 18s touted citizenship c...   \n",
              "\n",
              "    Label - (business, tech, politics, sport, entertainment)  Unnamed: 3  \\\n",
              "0                                                sport               NaN   \n",
              "1                                                 tech               NaN   \n",
              "2                                                sport               NaN   \n",
              "3                                             business               NaN   \n",
              "4                                                sport               NaN   \n",
              "..                                                 ...               ...   \n",
              "730                                           business               NaN   \n",
              "731                                      entertainment               NaN   \n",
              "732                                              sport               NaN   \n",
              "733                                           business               NaN   \n",
              "734                                           politics               NaN   \n",
              "\n",
              "     Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
              "0           NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "1           NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "2           NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "3           NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "4           NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "..          ...         ...         ...         ...         ...         ...   \n",
              "730         NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "731         NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "732         NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "733         NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "734         NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "\n",
              "     ...  Unnamed: 20  Unnamed: 21  Unnamed: 22  Unnamed: 23  Unnamed: 24  \\\n",
              "0    ...          NaN          NaN          NaN          NaN          NaN   \n",
              "1    ...          NaN          NaN          NaN          NaN          NaN   \n",
              "2    ...          NaN          NaN          NaN          NaN          NaN   \n",
              "3    ...          NaN          NaN          NaN          NaN          NaN   \n",
              "4    ...          NaN          NaN          NaN          NaN          NaN   \n",
              "..   ...          ...          ...          ...          ...          ...   \n",
              "730  ...          NaN          NaN          NaN          NaN          NaN   \n",
              "731  ...          NaN          NaN          NaN          NaN          NaN   \n",
              "732  ...          NaN          NaN          NaN          NaN          NaN   \n",
              "733  ...          NaN          NaN          NaN          NaN          NaN   \n",
              "734  ...          NaN          NaN          NaN          NaN          NaN   \n",
              "\n",
              "     Unnamed: 25  Unnamed: 26  Unnamed: 27  Unnamed: 28  Unnamed: 29  \n",
              "0            NaN          NaN          NaN          NaN        sport  \n",
              "1            NaN          NaN          NaN          NaN         tech  \n",
              "2            NaN          NaN          NaN          NaN        sport  \n",
              "3            NaN          NaN          NaN          NaN     business  \n",
              "4            NaN          NaN          NaN          NaN        sport  \n",
              "..           ...          ...          ...          ...          ...  \n",
              "730          NaN          NaN          NaN          NaN          NaN  \n",
              "731          NaN          NaN          NaN          NaN          NaN  \n",
              "732          NaN          NaN          NaN          NaN          NaN  \n",
              "733          NaN          NaN          NaN          NaN          NaN  \n",
              "734          NaN          NaN          NaN          NaN          NaN  \n",
              "\n",
              "[735 rows x 30 columns]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on0H41HSQ4pt",
        "outputId": "9bdde2c9-ea69-44ce-bc86-c0514da37968"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['ArticleId', 'Text',\n",
              "       'Label - (business, tech, politics, sport, entertainment)',\n",
              "       'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7',\n",
              "       'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12',\n",
              "       'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16',\n",
              "       'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20',\n",
              "       'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24',\n",
              "       'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27', 'Unnamed: 28',\n",
              "       'Unnamed: 29'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Nsb5dUW8Q3Xq"
      },
      "outputs": [],
      "source": [
        "df = df[['Text', 'Label - (business, tech, politics, sport, entertainment)']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Hb9azJoiQvdr"
      },
      "outputs": [],
      "source": [
        "tokenized_test = [nltk.word_tokenize(text.lower()) for text in df['Text']]\n",
        "sequences_test = [[vocab.get(word, 1) for word in seq] for seq in tokenized_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "4xnMeX4XQzws"
      },
      "outputs": [],
      "source": [
        "MAX_LEN_test = max(len(seq) for seq in sequences_test)\n",
        "padded_test = torch.zeros((len(sequences_test), MAX_LEN_test), dtype=torch.long)\n",
        "for i, seq in enumerate(sequences_test):\n",
        "    length = min(len(seq), MAX_LEN_test)\n",
        "    padded_test[i, :length] = torch.tensor(seq[:length])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "PQZNDNvjQ157"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "true_labels = le.fit_transform(df['Label - (business, tech, politics, sport, entertainment)'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "vdGtNvtQRCJP"
      },
      "outputs": [],
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "Pse8bTFvRCG3"
      },
      "outputs": [],
      "source": [
        "test_dataset = NewsDataset(padded_test, torch.tensor(true_labels, dtype=torch.long))\n",
        "BATCH_SIZE = 64\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6RSMCH_RCER",
        "outputId": "87ebe38a-5342-4115-ac18-dc5d8b75d044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CBiLSTM_WE_SA(\n",
              "  (embedding): Embedding(28180, 300)\n",
              "  (conv3): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
              "  (conv4): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
              "  (conv5): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
              "  (cnn_dropout): Dropout(p=0.5, inplace=False)\n",
              "  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
              "  (lstm_dropout): Dropout(p=0.5, inplace=False)\n",
              "  (embed_dense): Linear(in_features=300, out_features=128, bias=True)\n",
              "  (proj_cnn): Linear(in_features=300, out_features=100, bias=True)\n",
              "  (proj_lstm): Linear(in_features=256, out_features=100, bias=True)\n",
              "  (proj_embed): Linear(in_features=128, out_features=100, bias=True)\n",
              "  (self_attn): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
              "  )\n",
              "  (fc2): Linear(in_features=100, out_features=5, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = CBiLSTM_WE_SA(embedding_matrix, NUM_CLASSES).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/AIL721_A3/CBiLSTM_WE_SA.pth'))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "5jNmSa69RCBr"
      },
      "outputs": [],
      "source": [
        "all_preds = []\n",
        "all_true = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_true.extend(labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYfGdrFGRIeF",
        "outputId": "ea114f5e-a1aa-4959-9420-0ddbd6154537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.20594249055856598"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(all_true, all_preds, average='weighted')\n",
        "f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7PdxCoeSLUj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
